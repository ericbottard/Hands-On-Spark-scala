Spark Hands-on - Scala 
=============================================================================

In this Hands-On, we are going to show how you can use Apache Spark and some components of it ecosystem for data processing.
In the first part, you will implement the wordcount example with the Spark API, and play with some classic useful functions.

After that, we introduce Spark SQL, which allows relational queries expressed in SQL, HiveQL, or Scala to be executed using Spark. Here Spark SQL is used to analyse a dataset of tweets.

In the third part, we want to define the language of tweets. We use Spark MLLib to apply algorithms to train a model on the data and make prediction then.

To finish Spark Streaming is used with Spark MLLib to filter a live stream of Tweets for those that match some criteria.

Let's go and have fun !

## Prerequisites
- Java > 6 (8 is better to use the lambda expression)
- IDE

## Spark shell
The Spark shell is very useful for interactive analysis. For example you can use it when you want testing or play with the API.
Just launch it with the following command (for the Scala shell) :

`spark-shell`

## Part 1 : wordcount & text mining

## Part 2 : have fun with Spark SQL

## Part 3 : Machine Learning with MLlib

## Part 4 : Let's play with Spark streaming

# Some links
Apache Spark <https://spark.apache.org>

Ampcamp <http://ampcamp.berkeley.edu/>

Spark via Databricks <http://databricks.gitbooks.io/databricks-spark-reference-applications/>

Talks 

- <https://speakerdeck.com/nivdul/lightning-fast-machine-learning-with-spark>
- <https://speakerdeck.com/samklr/scalable-machine-learning-with-spark>

